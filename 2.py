#!/opt/mjjvm/mjjvm-venv/bin/python3
# -*- coding: utf-8 -*-
from __future__ import annotations

import os
import sys
import time
import json
import random
import logging
import warnings
from logging.handlers import RotatingFileHandler
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# network libs
import cloudscraper
import requests

# Playwright lazy import (may not be installed)
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except Exception:
    PLAYWRIGHT_AVAILABLE = False

# ---------------------------- é…ç½® ----------------------------
URLS = {
    "ç™½é“¶åŒº": "https://www.mjjvm.com/cart?fid=1&gid=1",
    "é»„é‡‘åŒº": "https://www.mjjvm.com/cart?fid=1&gid=2",
    "é’»çŸ³åŒº": "https://www.mjjvm.com/cart?fid=1&gid=3",
    "æ˜Ÿæ›œåŒº": "https://www.mjjvm.com/cart?fid=1&gid=4",
    "ç‰¹åˆ«æ´»åŠ¨åŒº": "https://www.mjjvm.com/cart?fid=1&gid=6",
}

ROOT_ORIGIN = "https://www.mjjvm.com"
INTERVAL = int(os.getenv("INTERVAL", "60"))
DATA_FILE = "stock_data.json"
LOG_FILE = "stock_out.log"

# ---------------------------- ç¯å¢ƒ / .env ----------------------------
load_dotenv()
SCKEY = os.getenv("SCKEY", "").strip()
MJJVM_COOKIE = os.getenv("MJJVM_COOKIE", "").strip()  # "PHPSESSID=...; cf_clearance=..."
PROXY = os.getenv("PROXY", "").strip()  # optional proxy like "http://user:pass@host:port"

# ---------------------------- æ—¥å¿— ----------------------------
warnings.filterwarnings("ignore", category=FutureWarning)
logger = logging.getLogger("StockMonitor")
logger.setLevel(logging.INFO)
fmt = logging.Formatter("[%(asctime)s] %(message)s", "%Y-%m-%d %H:%M:%S")
ch = logging.StreamHandler(stream=sys.stdout)
ch.setFormatter(fmt)
logger.addHandler(ch)
fh = RotatingFileHandler(LOG_FILE, maxBytes=2*1024*1024, backupCount=2, encoding="utf-8")
fh.setFormatter(fmt)
logger.addHandler(fh)

# ---------------------------- UA åˆ—è¡¨ ----------------------------
UAS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
]
def random_ua():
    return random.choice(UAS)

# ---------------------------- helpers ----------------------------
def parse_cookie_string(cookie_str: str) -> dict:
    out = {}
    if not cookie_str:
        return out
    for part in cookie_str.split(";"):
        part = part.strip()
        if not part:
            continue
        if "=" in part:
            k, v = part.split("=", 1)
            out[k.strip()] = v.strip()
    return out

def load_previous_data():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            logger.warning("è¯»å–æ—§æ•°æ®å¤±è´¥ï¼Œå¿½ç•¥æ—§æ–‡ä»¶ã€‚")
            return {}
    return {}

def save_data(data):
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def group_by_region(all_products):
    grouped = {}
    for key, info in all_products.items():
        region = info.get("region", "æœªçŸ¥åœ°åŒº")
        grouped.setdefault(region, []).append(info)
    return grouped

MEMBER_NAME_MAP = {1: "ç¤¾åŒºæˆå‘˜", 2: "ç™½é“¶ä¼šå‘˜", 3: "é»„é‡‘ä¼šå‘˜", 4: "é’»çŸ³ä¼šå‘˜", 5: "æ˜Ÿæ›œä¼šå‘˜"}

# ---------------------------- æ–¹ç³–æ¨é€ ----------------------------
def send_ftqq(messages):
    if not SCKEY or not messages:
        if not SCKEY:
            logger.info("SCKEY æœªé…ç½®ï¼Œè·³è¿‡æ–¹ç³–æ¨é€ã€‚")
        return
    url = f"https://sctapi.ftqq.com/{SCKEY}.send"
    for msg in messages:
        region = msg.get("region", "æœªçŸ¥åœ°åŒº")
        member_text = ""
        if msg.get("member_only", 0):
            member_text = f"è¦æ±‚ï¼š{MEMBER_NAME_MAP.get(msg['member_only'], 'ä¼šå‘˜')}\n"
        if msg["type"] == "ä¸Šæ¶":
            title = f"ğŸŸ¢ ä¸Šæ¶ - {region}"
        elif msg["type"] == "åº“å­˜å˜åŒ–":
            title = f"ğŸŸ¡ åº“å­˜å˜åŒ– - {region}"
        elif msg["type"] == "å”®ç½„":
            title = f"ğŸ”´ å”®ç½„ - {region}"
        else:
            title = f"âš ï¸ æŠ¥è­¦ - {region}"
        content = f"åç§°: {msg['name']}\nåº“å­˜: {msg['stock']}\n{member_text}{msg.get('config','')}\nç›´è¾¾é“¾æ¥: {msg['url']}"
        try:
            resp = requests.post(url, data={"title": title, "desp": content}, timeout=10)
            if resp.status_code == 200:
                logger.info("âœ… æ–¹ç³–æ¨é€æˆåŠŸ: %s", title)
            else:
                logger.error("âŒ æ–¹ç³–æ¨é€å¤±è´¥ %s: %s", resp.status_code, resp.text)
        except Exception as e:
            logger.error("âŒ æ–¹ç³–æ¨é€å¼‚å¸¸: %s", e)

# ---------------------------- cloudscraper session ----------------------------
def build_scraper():
    s = cloudscraper.create_scraper()
    if PROXY:
        s.proxies.update({"http": PROXY, "https": PROXY})
        logger.info("ä½¿ç”¨ä»£ç†ï¼š%s", PROXY)
    if MJJVM_COOKIE:
        cd = parse_cookie_string(MJJVM_COOKIE)
        for k, v in cd.items():
            try:
                s.cookies.set(k, v, domain="www.mjjvm.com")
            except Exception:
                s.cookies.set(k, v)
        logger.info("æ³¨å…¥ MJJVM_COOKIE: %s", ", ".join(list(cd.keys())))
    return s

# ---------------------------- Playwright fallback ----------------------------
def fetch_with_playwright(url: str, cookies: dict | None = None, wait_ms: int = 2500):
    """è¿”å› (html, cookies_dict) æˆ– (None, {})"""
    if not PLAYWRIGHT_AVAILABLE:
        logger.warning("Playwright æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨æŠ“å–ã€‚")
        return None, {}
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True, args=["--no-sandbox", "--disable-setuid-sandbox"])
            context = browser.new_context(user_agent=random_ua(), viewport={"width":1280,"height":800})
            if cookies:
                cookie_list = []
                for k, v in cookies.items():
                    cookie_list.append({"name": k, "value": v, "domain": "www.mjjvm.com", "path": "/", "httpOnly": False, "secure": True})
                try:
                    context.add_cookies(cookie_list)
                    logger.info("Playwright: æ³¨å…¥ cookiesï¼š%s", ", ".join(cookies.keys()))
                except Exception:
                    logger.debug("Playwright æ³¨å…¥ cookie å¤±è´¥ï¼ˆç»§ç»­ï¼‰", exc_info=True)
            page = context.new_page()
            page.goto(url, wait_until="networkidle", timeout=60000)
            page.wait_for_timeout(wait_ms)
            html = page.content()
            try:
                cw = context.cookies()
                browser_cookies = {c["name"]: c["value"] for c in cw if "www.mjjvm.com" in c.get("domain","")}
            except Exception:
                browser_cookies = {}
            browser.close()
            return html, browser_cookies
    except Exception as e:
        logger.exception("Playwright æŠ“å–å¤±è´¥: %s", e)
        return None, {}

# ---------------------------- é¡µé¢è§£æ ----------------------------
def parse_products(html, url, region):
    soup = BeautifulSoup(html, "html.parser")
    products = {}
    MEMBER_MAP = {"æˆå‘˜": 1, "ç™½é“¶ä¼šå‘˜": 2, "é»„é‡‘ä¼šå‘˜": 3, "é’»çŸ³ä¼šå‘˜": 4, "æ˜Ÿæ›œä¼šå‘˜": 5}
    for card in soup.select("div.card.cartitem"):
        name_tag = card.find("h4")
        if not name_tag:
            continue
        name = name_tag.get_text(strip=True)
        config_items = []
        member_only = 0
        for li in card.select("ul.vps-config li"):
            text = li.get_text(" ", strip=True)
            matched = False
            for key, value in MEMBER_MAP.items():
                if key in text:
                    member_only = value
                    matched = True
                    break
            if not matched:
                config_items.append(text)
        config = "\n".join(config_items)
        stock = 0
        stock_tag = card.find("p", class_="card-text")
        if stock_tag:
            try:
                stock = int(stock_tag.get_text(strip=True).split("åº“å­˜ï¼š")[-1])
            except Exception:
                stock = 0
        link_tag = card.select_one("div.card-footer a")
        pid = None
        if link_tag and "pid=" in link_tag.get("href", ""):
            pid = link_tag.get("href").split("pid=")[-1]
        products[f"{region} - {name}"] = {
            "name": name,
            "config": config,
            "stock": stock,
            "member_only": member_only,
            "url": url,
            "pid": pid,
            "region": region
        }
    return products

# ---------------------------- ä¸»å¾ªç¯ ----------------------------
consecutive_fail_rounds = 0

def main_loop():
    global consecutive_fail_rounds
    scraper = build_scraper()
    # å…ˆå°è¯•è®¿é—®æ ¹åŸŸï¼ˆè§¦å‘ challengeï¼‰
    try:
        r0 = scraper.get(ROOT_ORIGIN, headers={"User-Agent": random_ua(), "Referer": ROOT_ORIGIN}, timeout=20)
        logger.info("æ ¹åŸŸè®¿é—®è¿”å›ï¼š%s", getattr(r0, "status_code", None))
    except Exception as e:
        logger.warning("æ ¹åŸŸè®¿é—®å¼‚å¸¸ï¼š%s", e)

    prev_raw = load_previous_data()
    prev_data = {}
    for region, plist in prev_raw.items():
        for p in plist:
            prev_data[f"{region} - {p.get('name','')}"] = p

    logger.info("åº“å­˜ç›‘æ§å¯åŠ¨ï¼Œæ¯ %s ç§’æ£€æŸ¥ä¸€æ¬¡...", INTERVAL)

    while True:
        logger.info("æ­£åœ¨æ£€æŸ¥åº“å­˜...")
        all_products = {}
        success_count = 0
        fail_count = 0
        any_success = False

        for region, url in URLS.items():
            success_this_url = False
            last_err = None
            for attempt in range(3):
                headers = {"User-Agent": random_ua(), "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8", "Referer": ROOT_ORIGIN}
                try:
                    r = scraper.get(url, headers=headers, timeout=20)
                    status = getattr(r, "status_code", None)
                    if status == 403:
                        logger.warning("[%s] æ”¶åˆ° 403 (ç¬¬ %d æ¬¡å°è¯•)ã€‚", region, attempt+1)
                        last_err = "403"
                        time.sleep(2)
                        continue
                    r.raise_for_status()
                    prods = parse_products(r.text, url, region)
                    all_products.update(prods)
                    success_this_url = True
                    any_success = True
                    logger.info("[%s] è¯·æ±‚æˆåŠŸ (ç¬¬ %d æ¬¡å°è¯•)", region, attempt+1)
                    break
                except Exception as e:
                    last_err = e
                    logger.warning("[%s] è¯·æ±‚å¤±è´¥ (ç¬¬ %d æ¬¡å°è¯•): %s", region, attempt+1, e)
                    time.sleep(2)

            if not success_this_url:
                # fallback to Playwright
                if PLAYWRIGHT_AVAILABLE:
                    logger.info("[%s] cloudscraper å¤±è´¥ï¼Œå°è¯• Playwright æŠ“å–ã€‚", region)
                    cookie_dict = parse_cookie_string(MJJVM_COOKIE)
                    html, browser_cookies = fetch_with_playwright(url, cookies=cookie_dict)
                    if html:
                        # æŠŠæµè§ˆå™¨æ‹¿åˆ°çš„ cookie å›æ³¨åˆ° cloudscraperï¼Œæé«˜åç»­æˆåŠŸç‡
                        try:
                            for k, v in (browser_cookies or {}).items():
                                scraper.cookies.set(k, v, domain="www.mjjvm.com")
                            if browser_cookies:
                                logger.info("[%s] å°† Playwright æŠ“åˆ°çš„ cookies æ³¨å…¥ cloudscraper: %s", region, ", ".join(browser_cookies.keys()))
                        except Exception:
                            pass
                        prods = parse_products(html, url, region)
                        all_products.update(prods)
                        success_this_url = True
                        any_success = True
                        logger.info("[%s] Playwright æŠ“å–æˆåŠŸå¹¶è§£æã€‚", region)
                    else:
                        logger.warning("[%s] Playwright æŠ“å–å¤±è´¥æˆ–è¿”å›ç©ºã€‚", region)
                else:
                    logger.debug("Playwright ä¸å¯ç”¨ï¼Œè·³è¿‡æµè§ˆå™¨æŠ“å–ã€‚")

            if success_this_url:
                success_count += 1
            else:
                fail_count += 1
                logger.error("[%s] è¯·æ±‚å¤±è´¥: å°è¯• 3 æ¬¡å‡å¤±è´¥ (%s)", region, last_err)

        logger.info("æœ¬è½®è¯·æ±‚å®Œæˆ: æˆåŠŸ %d / %d, å¤±è´¥ %d", success_count, len(URLS), fail_count)

        if success_count == 0:
            consecutive_fail_rounds += 1
            logger.warning("æœ¬è½®å…¨éƒ¨è¯·æ±‚å¤±è´¥ï¼Œè¿ç»­å¤±è´¥è½®æ•°: %d", consecutive_fail_rounds)
        else:
            consecutive_fail_rounds = 0

        if consecutive_fail_rounds >= 10:
            logger.warning("è¿ç»­ %d è½®å…¨éƒ¨å¤±è´¥ï¼Œå‘é€æŠ¥è­¦ã€‚", consecutive_fail_rounds)
            send_ftqq([{"type": "æŠ¥è­¦", "name": "ç›‘æ§", "stock": 0, "region": "ç³»ç»Ÿ", "url": ROOT_ORIGIN}])
            consecutive_fail_rounds = 0

        if not any_success:
            logger.warning("æœ¬è½®è¯·æ±‚å…¨éƒ¨å¤±è´¥ï¼Œè·³è¿‡æ•°æ®æ›´æ–°ã€‚")
            time.sleep(INTERVAL)
            continue

        messages = []
        for name, info in all_products.items():
            if info.get("member_only", 0) == 0:
                continue
            prev_stock = prev_data.get(name, {}).get("stock", 0)
            curr_stock = info.get("stock", 0)
            msg_type = None
            if prev_stock == 0 and curr_stock > 0:
                msg_type = "ä¸Šæ¶"
            elif prev_stock > 0 and curr_stock == 0:
                msg_type = "å”®ç½„"
            elif prev_stock != curr_stock:
                msg_type = "åº“å­˜å˜åŒ–"
            if msg_type:
                msg = {
                    "type": msg_type,
                    "name": info["name"],
                    "stock": curr_stock,
                    "config": info.get("config", ""),
                    "member_only": info.get("member_only", 0),
                    "url": info.get("url", ROOT_ORIGIN),
                    "region": info.get("region", "æœªçŸ¥åœ°åŒº")
                }
                messages.append(msg)
                logger.info("%s - %s  |  åº“å­˜: %s  |  %s", msg_type, info["name"], curr_stock, MEMBER_NAME_MAP.get(info.get("member_only", 0), "ä¼šå‘˜"))

        if messages:
            send_ftqq(messages)

        try:
            save_data(group_by_region(all_products))
        except Exception as e:
            logger.warning("ä¿å­˜æ•°æ®å¤±è´¥: %s", e)

        prev_data = all_products
        time.sleep(INTERVAL)

# ---------------------------- å¯åŠ¨ ----------------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="MJJVM ç›‘æ§ (cloudscraper + playwright fallback)")
    parser.add_argument("--test", action="store_true", help="å‘é€æµ‹è¯•æ¨é€åé€€å‡º")
    args = parser.parse_args()
    if args.test:
        send_ftqq([{"type": "ä¸Šæ¶", "name": "æµ‹è¯•å•†å“", "stock": 10, "config": "2C/2G", "member_only": 2, "url": ROOT_ORIGIN, "region": "æµ‹è¯•åŒº"}])
        logger.info("âœ… æµ‹è¯•æ¨é€å·²å‘é€")
        sys.exit(0)
    main_loop()
