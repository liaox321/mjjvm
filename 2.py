#!/opt/mjjvm/mjjvm-venv/bin/python3
# -*- coding: utf-8 -*-
"""
MJJVM ç›‘æ§è„šæœ¬ï¼ˆå¢å¼º Cloudflare å…¼å®¹ï¼‰
ç‰¹æ€§ï¼š
- ä½¿ç”¨ cloudscraper è‡ªåŠ¨å¤„ç† Cloudflare JS challenge
- å¯é€‰ä» .env è¯»å– MJJVM_COOKIEï¼ˆå¦‚æœä½ æ‰‹åŠ¨ä»æµè§ˆå™¨å¤åˆ¶äº† cookieï¼‰
- å¯é€‰é€šè¿‡ PROXY ç¯å¢ƒå˜é‡ä½¿ç”¨ä»£ç†
- éšæœº User-Agentï¼ˆå¤šç§å¸¸è§æµè§ˆå™¨ UAï¼‰
- ä¿ç•™æ–¹ç³–æ¨é€ã€åº“å­˜æ¯”å¯¹ä¸æ—¥å¿—
"""
from __future__ import annotations
import cloudscraper
from bs4 import BeautifulSoup
import time
import json
import os
import sys
import logging
from logging.handlers import RotatingFileHandler
import warnings
from dotenv import load_dotenv
import random
import requests

# ---------------------------- é…ç½® ----------------------------
URLS = {
    "ç™½é“¶åŒº": "https://www.mjjvm.com/cart?fid=1&gid=1",
    "é»„é‡‘åŒº": "https://www.mjjvm.com/cart?fid=1&gid=2",
    "é’»çŸ³åŒº": "https://www.mjjvm.com/cart?fid=1&gid=3",
    "æ˜Ÿè€€åŒº": "https://www.mjjvm.com/cart?fid=1&gid=4",
    "ç‰¹åˆ«æ´»åŠ¨åŒº": "https://www.mjjvm.com/cart?fid=1&gid=6",
}

INTERVAL = int(os.getenv("INTERVAL", "60"))  # ç§’ï¼Œå¾ªç¯é—´éš”ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
DATA_FILE = "stock_data.json"
LOG_FILE = "stock_out.log"
ROOT_ORIGIN = "https://www.mjjvm.com"

# ---------------------------- åŠ è½½ .env ----------------------------
load_dotenv()
SCKEY = os.getenv("SCKEY", "").strip()
MJJVM_COOKIE = os.getenv("MJJVM_COOKIE", "").strip()  # å¯é€‰ï¼šPHPSESSID=...; cf_clearance=...
PROXY = os.getenv("PROXY", "").strip()  # å¯é€‰ï¼šhttp://user:pass@host:port
# å¯é€‰ï¼šè‹¥ä½ åªæƒ³ä¸´æ—¶æµ‹è¯•ï¼Œä¸å‘é€æ–¹ç³–æ¨é€ï¼ŒSCKEY ç•™ç©ºå³å¯

# ---------------------------- æ—¥å¿— ----------------------------
warnings.filterwarnings("ignore", category=FutureWarning)
logger = logging.getLogger("StockMonitor")
logger.setLevel(logging.INFO)
formatter = logging.Formatter("[%(asctime)s] %(message)s", "%Y-%m-%d %H:%M:%S")
console_handler = logging.StreamHandler(stream=sys.stdout)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
file_handler = RotatingFileHandler(LOG_FILE, maxBytes=2*1024*1024, backupCount=2, encoding="utf-8")
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# ---------------------------- User-Agent åˆ—è¡¨ï¼ˆè½®æ¢ï¼‰ ----------------------------
UAS = [
    # ä¸»æµæ¡Œé¢ UA
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",
    # å¸¸è§ç§»åŠ¨ UAï¼ˆè‹¥éœ€è¦ï¼‰
    "Mozilla/5.0 (Linux; Android 13; Pixel 6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36",
]

def random_ua() -> str:
    return random.choice(UAS)

# ---------------------------- å·¥å…·å‡½æ•° ----------------------------
def load_previous_data():
    if os.path.exists(DATA_FILE):
        try:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            logger.warning("æ— æ³•è¯»å–æ—§æ•°æ®æ–‡ä»¶ï¼Œé‡å»ºã€‚")
            return {}
    return {}

def save_data(data):
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def group_by_region(all_products):
    grouped = {}
    for key, info in all_products.items():
        region = info.get("region", "æœªçŸ¥åœ°åŒº")
        grouped.setdefault(region, []).append(info)
    return grouped

MEMBER_NAME_MAP = {
    1: "ç¤¾åŒºæˆå‘˜",
    2: "ç™½é“¶ä¼šå‘˜",
    3: "é»„é‡‘ä¼šå‘˜",
    4: "é’»çŸ³ä¼šå‘˜",
    5: "æ˜Ÿæ›œä¼šå‘˜"
}

# ---------------------------- æ–¹ç³–æ¨é€ ----------------------------
def send_ftqq(messages):
    if not messages or not SCKEY:
        if not SCKEY:
            logger.info("SCKEY æœªé…ç½®ï¼Œè·³è¿‡æ–¹ç³–æ¨é€ã€‚")
        return
    url = f"https://sctapi.ftqq.com/{SCKEY}.send"
    for msg in messages:
        region = msg.get("region", "æœªçŸ¥åœ°åŒº")
        member_text = ""
        if msg.get("member_only", 0):
            member_name = MEMBER_NAME_MAP.get(msg["member_only"], "ä¼šå‘˜")
            member_text = f"è¦æ±‚ï¼š{member_name}\n"
        if msg["type"] == "ä¸Šæ¶":
            title = f"ğŸŸ¢ ä¸Šæ¶ - {region}"
        elif msg["type"] == "åº“å­˜å˜åŒ–":
            title = f"ğŸŸ¡ åº“å­˜å˜åŒ– - {region}"
        elif msg["type"] == "å”®ç½„":
            title = f"ğŸ”´ å”®ç½„ - {region}"
        else:
            title = f"âš ï¸ æŠ¥è­¦ - {region}"
        content = f"""
åç§°: {msg['name']}
åº“å­˜: {msg['stock']}
{member_text}
{msg.get('config', '')}
ç›´è¾¾é“¾æ¥: {msg['url']}
""".strip()
        try:
            resp = requests.post(url, data={"title": title, "desp": content}, timeout=10)
            if resp.status_code == 200:
                logger.info("âœ… æ–¹ç³–æ¨é€æˆåŠŸ: %s", title)
            else:
                logger.error("âŒ æ–¹ç³–æ¨é€å¤±è´¥ %s: %s", resp.status_code, resp.text)
        except Exception as e:
            logger.error("âŒ æ–¹ç³–æ¨é€å¼‚å¸¸: %s", e)

# ---------------------------- Cloudscraper Session åˆå§‹åŒ– ----------------------------
def build_scraper() -> cloudscraper.CloudScraper:
    # ä½¿ç”¨ cloudscraper.create_scraper() åˆ›å»º session
    try:
        session = cloudscraper.create_scraper()
    except Exception:
        session = cloudscraper.create_scraper()  # fallback
    # è‹¥æä¾›ä»£ç†ï¼Œè®¾ç½®åˆ° sessionï¼ˆcloudscraper åŸºäº requestsï¼‰
    if PROXY:
        session.proxies.update({"http": PROXY, "https": PROXY})
        logger.info("ä½¿ç”¨ä»£ç†ï¼š%s", PROXY)
    return session

def parse_cookie_string(cookie_str: str) -> dict:
    """æŠŠ 'k=v; k2=v2' è½¬æˆ dict"""
    out = {}
    if not cookie_str:
        return out
    parts = cookie_str.split(";")
    for p in parts:
        p = p.strip()
        if not p:
            continue
        if "=" in p:
            k, v = p.split("=", 1)
            out[k.strip()] = v.strip()
    return out

def prepare_session(session: cloudscraper.CloudScraper):
    """
    1) å¦‚æœ MJJVM_COOKIE åœ¨ .envï¼Œæ³¨å…¥åˆ° session
    2) è®¿é—®æ ¹åŸŸè§¦å‘ cloudscraper è‡ªåŠ¨è§£ challengeï¼Œä¿å­˜ cf_clearance ç­‰ cookie
    """
    # æ³¨å…¥ç”¨æˆ·æä¾›çš„ cookieï¼ˆå¯é€‰ï¼‰
    if MJJVM_COOKIE:
        cookie_dict = parse_cookie_string(MJJVM_COOKIE)
        for k, v in cookie_dict.items():
            session.cookies.set(k, v, domain="www.mjjvm.com")
        logger.info("å·²æ³¨å…¥ MJJVM_COOKIEï¼ˆæ¥è‡ª .envï¼‰: %s", ", ".join(cookie_dict.keys()))
    # è®¿é—®æ ¹åŸŸä»¥è§¦å‘ cloudflare challengeï¼ˆcloudscraper ä¼šå¤„ç†ï¼‰
    headers = {
        "User-Agent": random_ua(),
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Referer": ROOT_ORIGIN
    }
    try:
        r = session.get(ROOT_ORIGIN, headers=headers, timeout=20)
        logger.info("æ ¹åŸŸè®¿é—®è¿”å›ï¼š%s", getattr(r, "status_code", "NA"))
        if getattr(r, "status_code", None) and r.status_code >= 400:
            logger.debug("æ ¹åŸŸå“åº”å¤´ï¼ˆéƒ¨åˆ†ï¼‰: %s", dict(list(r.headers.items())[:10]))
    except Exception as e:
        logger.warning("è®¿é—®æ ¹åŸŸä»¥è·å– cf cookie å¤±è´¥: %s", e)

# ---------------------------- é¡µé¢è§£æ ----------------------------
def parse_products(html, url, region):
    soup = BeautifulSoup(html, "html.parser")
    products = {}
    MEMBER_MAP = {
        "æˆå‘˜": 1,
        "ç™½é“¶ä¼šå‘˜": 2,
        "é»„é‡‘ä¼šå‘˜": 3,
        "é’»çŸ³ä¼šå‘˜": 4,
        "æ˜Ÿæ›œä¼šå‘˜": 5,
    }
    for card in soup.select("div.card.cartitem"):
        name_tag = card.find("h4")
        if not name_tag:
            continue
        name = name_tag.get_text(strip=True)
        config_items = []
        member_only = 0
        for li in card.select("ul.vps-config li"):
            text = li.get_text(" ", strip=True)
            matched = False
            for key, value in MEMBER_MAP.items():
                if key in text:
                    member_only = value
                    matched = True
                    break
            if not matched:
                config_items.append(text)
        config = "\n".join(config_items)
        stock_tag = card.find("p", class_="card-text")
        stock = 0
        if stock_tag:
            try:
                stock = int(stock_tag.get_text(strip=True).split("åº“å­˜ï¼š")[-1])
            except Exception:
                stock = 0
        link_tag = card.select_one("div.card-footer a")
        pid = None
        if link_tag and "pid=" in link_tag.get("href", ""):
            pid = link_tag["href"].split("pid=")[-1]
        products[f"{region} - {name}"] = {
            "name": name,
            "config": config,
            "stock": stock,
            "member_only": member_only,
            "url": url,
            "pid": pid,
            "region": region
        }
    return products

# ---------------------------- ä¸»å¾ªç¯ ----------------------------
consecutive_fail_rounds = 0

def main_loop():
    global consecutive_fail_rounds
    scraper = build_scraper()
    prepare_session(scraper)  # è®© cloudscraper å°è¯•æ‹¿åˆ° cf cookie / session

    prev_data_raw = load_previous_data()
    prev_data = {}
    for region, plist in prev_data_raw.items():
        for p in plist:
            prev_data[f"{region} - {p.get('name','')}"] = p

    logger.info("åº“å­˜ç›‘æ§å¯åŠ¨ï¼Œæ¯ %s ç§’æ£€æŸ¥ä¸€æ¬¡...", INTERVAL)

    while True:
        logger.info("æ­£åœ¨æ£€æŸ¥åº“å­˜...")
        all_products = {}
        success_count = 0
        fail_count = 0
        success = False

        for region, url in URLS.items():
            success_this_url = False
            last_err = None
            for attempt in range(3):
                headers = {
                    "User-Agent": random_ua(),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "zh-CN,zh;q=0.9",
                    "Referer": ROOT_ORIGIN
                }
                try:
                    r = scraper.get(url, headers=headers, timeout=20)
                    # å¦‚æœæœåŠ¡å™¨è¿”å› 403ï¼Œä½†å¸¦æœ‰ cloudflare æŒ‡ç¤ºå¤´ï¼Œè®°å½•å¹¶å°è¯•é‡æ–° prepare_session
                    status = getattr(r, "status_code", None)
                    if status == 403:
                        logger.warning("[%s] æ”¶åˆ° 403 (ç¬¬ %d æ¬¡å°è¯•)ï¼Œä¼šæ‰“å°éƒ¨åˆ†å“åº”å¤´å¸®åŠ©æ’æŸ¥ã€‚", region, attempt+1)
                        # æ‰“å°å°‘é‡å“åº”å¤´ä¾¿äºåˆ†æï¼ˆä¸æš´éœ²å¤ªå¤šï¼‰
                        try:
                            logger.debug("[%s] resp.headers (sample): %s", region, dict(list(r.headers.items())[:8]))
                            # å¦‚æœæŒ‘æˆ˜å‹ header å‡ºç°ï¼Œå°è¯•é‡æ–° prepare sessionï¼ˆè§¦å‘ cloudscraperï¼‰
                            if any(k.lower().startswith("cf-") or "challenge" in k.lower() for k in r.headers.keys()):
                                logger.info("[%s] æ£€æµ‹åˆ° Cloudflare ç›¸å…³å¤´ï¼Œtrigger prepare_session é‡è¯•ã€‚", region)
                                prepare_session(scraper)
                        except Exception:
                            pass
                        last_err = f"403 {r.headers.get('server','')}"
                        time.sleep(2)
                        continue
                    r.raise_for_status()
                    products = parse_products(r.text, url, region)
                    all_products.update(products)
                    success_this_url = True
                    logger.info("[%s] è¯·æ±‚æˆåŠŸ (ç¬¬ %d æ¬¡å°è¯•)", region, attempt + 1)
                    break
                except Exception as e:
                    last_err = e
                    logger.warning("[%s] è¯·æ±‚å¤±è´¥ (ç¬¬ %d æ¬¡å°è¯•): %s", region, attempt + 1, e)
                    # å½“é‡åˆ°å¯èƒ½ä¸ cf ç›¸å…³çš„é”™è¯¯ï¼Œå°è¯•çŸ­æš‚ç­‰å¾…å¹¶é‡æ–° prepare
                    time.sleep(2)
                    try:
                        prepare_session(scraper)
                    except Exception:
                        pass

            if success_this_url:
                success = True
                success_count += 1
            else:
                fail_count += 1
                logger.error("[%s] è¯·æ±‚å¤±è´¥: å°è¯• 3 æ¬¡å‡å¤±è´¥ (%s)", region, last_err)

        logger.info("æœ¬è½®è¯·æ±‚å®Œæˆ: æˆåŠŸ %d / %d, å¤±è´¥ %d", success_count, len(URLS), fail_count)

        if success_count == 0:
            consecutive_fail_rounds += 1
            logger.warning("æœ¬è½®å…¨éƒ¨è¯·æ±‚å¤±è´¥ï¼Œè¿ç»­å¤±è´¥è½®æ•°: %d", consecutive_fail_rounds)
        else:
            consecutive_fail_rounds = 0

        if consecutive_fail_rounds >= 10:
            logger.warning("è¿ç»­ %d è½®å…¨éƒ¨å¤±è´¥ï¼Œå‘é€æŠ¥è­¦å¹¶é‡ç½®è®¡æ•°ã€‚", consecutive_fail_rounds)
            send_ftqq([{"type": "æŠ¥è­¦", "name": "ç›‘æ§", "stock": 0, "region": "ç³»ç»Ÿ", "url": ROOT_ORIGIN}])
            consecutive_fail_rounds = 0

        if not success:
            logger.warning("æœ¬è½®è¯·æ±‚å…¨éƒ¨å¤±è´¥ï¼Œè·³è¿‡æ•°æ®æ›´æ–°ã€‚")
            time.sleep(INTERVAL)
            continue

        # æ„é€ æ¶ˆæ¯ï¼ˆåªå¯¹ member_only é 0 çš„å•†å“å‘é€šçŸ¥ï¼‰
        messages = []
        for name, info in all_products.items():
            if info.get("member_only", 0) == 0:
                continue
            prev_stock = prev_data.get(name, {}).get("stock", 0)
            curr_stock = info.get("stock", 0)
            msg_type = None
            if prev_stock == 0 and curr_stock > 0:
                msg_type = "ä¸Šæ¶"
            elif prev_stock > 0 and curr_stock == 0:
                msg_type = "å”®ç½„"
            elif prev_stock != curr_stock:
                msg_type = "åº“å­˜å˜åŒ–"
            if msg_type:
                msg = {
                    "type": msg_type,
                    "name": info["name"],
                    "stock": curr_stock,
                    "config": info.get('config', ''),
                    "member_only": info.get("member_only", 0),
                    "url": info.get("url", ROOT_ORIGIN),
                    "region": info.get("region", "æœªçŸ¥åœ°åŒº")
                }
                messages.append(msg)
                member_name = MEMBER_NAME_MAP.get(info.get("member_only", 0), "ä¼šå‘˜")
                logger.info("%s - %s  |  åº“å­˜: %s  |  %s", msg_type, info["name"], curr_stock, member_name)

        if messages:
            send_ftqq(messages)

        grouped_data = group_by_region(all_products)
        try:
            save_data(grouped_data)
        except Exception as e:
            logger.warning("ä¿å­˜æ•°æ®å¤±è´¥: %s", e)
        prev_data = all_products

        time.sleep(INTERVAL)

# ---------------------------- å¯åŠ¨ ----------------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="MJJVM ç›‘æ§è„šæœ¬ (Cloudflare å…¼å®¹å¢å¼ºç‰ˆ)")
    parser.add_argument("--test", action="store_true", help="å‘é€ä¸€æ¡æµ‹è¯•æ¨é€åé€€å‡º")
    args = parser.parse_args()
    if args.test:
        send_ftqq([{
            "type": "ä¸Šæ¶",
            "name": "æµ‹è¯•å•†å“",
            "stock": 10,
            "config": "2C/2G",
            "member_only": 2,
            "url": ROOT_ORIGIN,
            "region": "æµ‹è¯•åŒº"
        }])
        logger.info("âœ… æµ‹è¯•æ¨é€å·²å‘é€")
        sys.exit(0)
    main_loop()
